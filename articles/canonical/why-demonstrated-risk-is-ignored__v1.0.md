# Why Demonstrated Risk Is Ignored in Large Organisations — and How to Fix That  
*Field notes for anyone who has ever pointed at the burning building and been told to “raise a ticket.”*

There’s a comforting myth that runs through large organisations:

> If you find a real risk, and you show the evidence clearly enough, rational people will act.

It’s a good story. It is also, far too often, false.

Large organisations rarely fail because risk is unknown.  
They fail because **known, demonstrated risks are structurally difficult to act on**.

This is not usually a matter of ignorance or bad faith. It is a consequence of how responsibility, authority, and accountability are arranged. In many systems, **ignoring reality is the least painful option for the people required to respond to it** [1][2].

I learned this through direct experience in government environments: years spent doing exactly what security frameworks prescribe—observe, verify, document, escalate—only to watch serious, demonstrable risk quietly sink into organisational sediment.

Sometimes the failures were trivial. Sometimes they were severe. The response, however, was remarkably consistent: acknowledgement without movement.

This is not a complaint piece. It is a design analysis.

Once the pattern becomes visible, one thing is clear:

**This is not primarily a people problem. It is a systems problem.**

And systems problems can be fixed.

## The myth of “evidence leads to action”

Most governance models assume a linear progression:

1. Risk exists  
2. Risk is identified  
3. Evidence is presented  
4. Decision-makers act  

In practice, there is an unspoken intermediate step:

**3.5. People assess what the evidence will do to them.**

Organisational research consistently shows that speaking up about problems is experienced as a form of interpersonal and professional risk unless the system explicitly protects it [1][2]. When raising an issue threatens reputation, career prospects, or institutional legitimacy, silence becomes rational—even when the risk is real.

This effect is amplified in large, regulated organisations. Decades of whistleblowing research demonstrate that individuals who surface serious problems frequently experience retaliation or career damage, even when their disclosures are accurate and well intentioned [3][4].

Truth, in such environments, is not merely information.  
Truth is a **force**.  
And forces meet **structures**.

## Four structural reasons demonstrated risk is ignored

Across sectors and domains, the same structural inhibitors recur.

### 1. Responsibility without authority

People are often tasked with identifying risk while lacking authority over budgets, roadmaps, deployment decisions, or escalation pathways.

They can discover risk, but they cannot compel remediation.

The predictable outcome is stasis:
- findings become tickets  
- tickets become backlog items  
- backlogs become archives of unacted-on truth  

**Evidence without authority is inert.**

This condition frequently produces what is labelled burnout but more accurately reflects **moral injury**: the psychological harm that arises when individuals are held responsible for outcomes they are structurally prevented from influencing [9][10].

### 2. Risk discovery as a career threat

In many organisations, discovering risk is implicitly treated as creating exposure rather than providing value.

Demonstrated risk threatens reputations, audit narratives, and prior decisions. As a result, systems evolve that quietly reward not knowing too much.

This is not anecdotal. Large comparative studies of whistleblowing in public-sector organisations, including Australian contexts, show that retaliation remains common even when disclosures are substantiated [4].

The messenger becomes the liability.

### 3. Compliance substituting for reality

Security and governance practices frequently prioritise assurance artefacts—policies, training records, risk registers—over empirical validation.

These artefacts provide social and political reassurance, but they do not necessarily correspond to real-world safety. Academic work on security theatre shows how symbolic compliance can actively suppress detection of genuine vulnerabilities by displacing testing and adversarial evaluation [7].

If a control cannot survive demonstration, it does not meaningfully exist.

### 4. Diffuse accountability

In complex organisations, responsibility is often distributed so broadly that no single actor is required to act.

Research in information security behaviour demonstrates that diffusion of responsibility significantly reduces corrective action, particularly in large systems with layered governance [5].

Everyone can see the problem.  
No one owns the outcome.

## Why demonstration can reduce action

There is a paradox at the heart of many security failures.

Demonstration should increase urgency. Sometimes it does. In many cases, it has the opposite effect.

Demonstrated risk collapses plausible deniability. Once a vulnerability is undeniable, organisations face increased legal, reputational, and financial exposure. Security economics research shows that this shift in incentives can delay or suppress action rather than accelerate it [6].

When remediation implies blame or liability, silence becomes locally rational.

This is why the same experience is reported again and again:

> “I showed them. I proved it. And nothing happened.”

This is not a failure of communication. It is a predictable outcome of incentive design.

## Not primarily a culture problem

Culture matters, but culture is downstream of structure.

Psychological safety, learning behaviour, and ethical action reliably emerge where authority aligns with responsibility and where risk discovery is protected rather than punished [1][2].

Where these conditions are absent, values statements and awareness training have limited effect.

Culture is the smell of the machine.  
Structure is the machine.

## Engineering systems where truth can move

Research across organisational learning, security economics, and high-reliability systems converges on a small number of effective interventions.

1. **Couple evidence with mandate**  
   Demonstrated risk should trigger predefined responses, timelines, and escalation paths.

2. **Protect risk discovery**  
   High-reliability systems treat reporting as a protected activity, not a threat [1][3].

3. **Replace assurance with empiricism**  
   Continuous testing and adversarial validation outperform static compliance models [6][7].

4. **Create no-ignore artefacts**  
   Evidence must be reproducible, attributable, and legible to decision-makers.

5. **Assign executive ownership of risk acceptance**  
   Explicit ownership aligns authority with consequence and changes decision behaviour [6].

## Moral injury and epistemic harm

When individuals who are calibrated to risk are forced to participate in denial or inaction, the result is not simply stress.

It is **moral injury**: harm arising from the betrayal of core ethical expectations by legitimate authority in high-stakes situations [9][10].

This phenomenon is well studied in military and healthcare settings and is increasingly recognised in organisational contexts.

The implication is straightforward:

You do not need more resilience.  
You need systems that do not punish reality.

## Conclusion

The question is not why organisations ignore demonstrated risk.

The question is why we continue to build organisations in which ignoring reality is rational.

Safer technical systems, safer AI systems, and safer institutions will not be produced by better intentions.

They will be produced by **architectures that force truth to move**.

If you are someone who sees failure modes before they become incidents or headlines, you are not the problem.

You are a sensor.

Just do not stay in buildings that punish their smoke alarms.

## Endnotes

[1] Edmondson, A. (1999). *Psychological safety and learning behavior in work teams.* Administrative Science Quarterly. https://journals.sagepub.com/doi/10.2307/2666999

[2] Edmondson, A. (2002). *Managing the risk of learning.* Harvard Business School Working Paper. https://hbswk.hbs.edu/item/managing-the-risk-of-learning

[3] Near, J. P., & Miceli, M. P. (1996). *Whistle-blowing: Myth and reality.* Journal of Management. https://journals.sagepub.com/doi/10.1177/014920639602200305

[4] Miceli, M. P., Near, J. P., & Dworkin, T. M. (2013). *Whistle-blowing in organizations.* Routledge. https://www.routledge.com/Whistle-Blowing-in-Organizations/Miceli-Near-Dworkin/p/book/9780415889324

[5] Hadlington, L. (2021). *Moral disengagement and information security awareness.* Computers in Human Behavior. https://doi.org/10.1016/j.chb.2020.106388

[6] Arora, A., Telang, R., & Xu, H. (2008). *Optimal policy for software vulnerability disclosure.* Management Science. https://doi.org/10.1287/mnsc.1070.0785

[7] Fassl, A. (2024). *Security theatre: Investigating symbolic security practices.* Doctoral dissertation. https://doi.org/10.17863/CAM.109700

[8] Schneier, B. *Security Theater.* https://www.schneier.com/essays/archives/2007/01/security_theater.html

[9] Shay, J. (2014). *Moral injury.* Psychoanalytic Psychology. https://psycnet.apa.org/record/2014-05815-001

[10] Molendijk, T. (2025). *Moral injury in organisations.* Social Science & Medicine. https://doi.org/10.1016/j.socscimed.2024.116138
